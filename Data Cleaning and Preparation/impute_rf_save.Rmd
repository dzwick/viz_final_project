---
title: "Imputing Missing Values in the College Scorecard Data Via Random Forest"
author: "Julia Silge"
date: "`r Sys.Date()`"
output:
  html_document:
    highlight: pygments
    theme: paper
    toc: yes
---

```{r, echo = FALSE, warning = FALSE}
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, dpi = 180)
options(width=100, dplyr.width = 150)
```

Let's work on imputing missing values in the [College Scorecard data]((https://collegescorecard.ed.gov/data/)) so we can do clustering. This time, let's try using random forest.

## Loading the Data and Counting the Missing Values

Let's open up the filtered, processed version of the College Scorecard data we have worked on together and settled on using at this point.

```{r}
library(readr)
colleges <- read_csv("data/scorecard_reduced_features.csv", na = c("NULL", "PrivacySuppressed"))
dim(colleges)
```

How many `NA` values do we have in each column?

```{r}
sapply(colleges, function(x) sum(is.na(x)))
sapply(colleges, function(x) mean(is.na(x)))
```

Most of those are not so bad, many around 15% or less.

## Substituting Missing Values Using Random Forest

Let's try imputing the missing values using the [missForest](https://cran.r-project.org/web/packages/missForest/index.html) package, and implementation of random forest for missing value imputation.

```{r}
library(missForest)
colleges[,c(4:18,23)] <- lapply(colleges[,c(4:18,23)], as.factor)
colleges_subset <- colleges[4:59]
impute_results <- missForest(as.data.frame(colleges_subset), verbose = TRUE)
```

Did this work?

```{r}
sapply(as.data.frame(impute_results$ximp), function(x) mean(is.na(x)))
```

What is the estimate of the OOB (out-of-bag) imputation error (normalized root mean squared error)?

```{r}
impute_results$OOBerror
```

## Saving Imputed Results to File

Let's save these results with imputed values replaced for the missing values to a CSV for later use.

```{r}
colleges_imputed <- as.data.frame(impute_results$ximp)
write_csv(colleges_imputed, "./scorecard_imputed.csv")
```

## Clustering Results with Random Forest Imputation

Let's do k-means clustering now and see what kind of results we get. Let's use the broom package to explore how many clusters may be appropriate for this data set. For starters, let's try out 4 to 20 clusters.

```{r}
library(dplyr)
library(broom)
library(tidyr)

set.seed(1234)
kclusts <- data.frame(k=4:20) %>% 
    group_by(k) %>% 
    do(kclust = kmeans(colleges_imputed, .$k))
```

Now let's use `tidy`, `augment`, and `glance` to organize these various sets of k-means clusterings.

```{r}
clusters <- kclusts %>% group_by(k) %>% do(tidy(.$kclust[[1]]))
assignments <- kclusts %>% group_by(k) %>% do(augment(.$kclust[[1]], colleges_imputed))
clusterings <- kclusts %>% group_by(k) %>% do(glance(.$kclust[[1]]))
```

Specifically, let's look at the total within sum of squares, saved in the `tot.withinss` column in `clusterings`. This shows us the variance within the clusters.

```{r, fig.width=8, fig.height=6}
library(ggfortify)
ggplot(clusterings, aes(k, tot.withinss)) +
    geom_line(color = "blue", alpha = 0.5, size = 2) +
    geom_point(size = 0.8)
```

Notice that the variance decreases with more clusters, but there is a "bend" or "elbow" around 12 clusters. This is around the same place we saw an elbow in the early subset of colleges, and tells us we don't gain much by going beyond 12 or so clusters. 

## Plotting Clustering Results

Let's look at how the colleges are distributed in the space of principal component 1 and 2, clustered with $k = 12$.

```{r, fig.width=10, fig.height=8}
colleges_imputed[] <- lapply(colleges_imputed[], as.numeric)
set.seed(1234)
autoplot(kmeans(colleges_imputed, 12), data = colleges_imputed, size = 3, alpha = 0.5) + 
    ggtitle("K-Means Clustering of College Scorecard Data") +
    theme(legend.position="none")
```

That looks REMARKABLY like the results from the clustering imputed with medians. Let's label a few of these.

```{r, fig.width=10, fig.height=8}
library(ggrepel)
set.seed(1234)
my_kmeans <- kmeans(colleges_imputed, 12)
autoplot(my_kmeans, data = colleges_imputed, size = 3, alpha = 0.5) + 
    ggtitle("K-Means Clustering of College Scorecard Data") +
    theme(legend.position="none") +
    geom_text_repel(aes(PC1, PC2, 
                        color = factor(my_kmeans$cluster), 
                        label = ifelse(SAT_AVG_ALL > 1475, colleges$INSTNM,'')),
                    segment.color = NA)
```

So it turns out the clustering is a little bit different now than before.

## Summary

It's important to remember that this fancier imputation method is still not adding any new information to the data set; it's just a matter of which is a more appropriate way of filling in `NA` values. Let's look, just for kicks, at a few of these clusters.

```{r}
colleges <- colleges %>% 
    mutate(cluster = my_kmeans$cluster) 

colleges %>%
    filter(cluster == 4) %>%
    select(INSTNM)

colleges %>%
    filter(cluster == 7) %>%
    select(INSTNM)

colleges %>%
    filter(cluster == 9) %>%
    select(INSTNM)

```

Let's look at what are the most important variables for principal component analysis, since that is so related to k-means.

```{r, fig.width=6, fig.height=9}
library(reshape2)

colleges_imputed <- scale(colleges_imputed)
college_pca <- prcomp(colleges_imputed)
melted <- melt(college_pca$rotation[,1:9])

ggplot(data = melted[melted$Var2 == "PC1",]) +
    theme(legend.position = "none", 
          axis.ticks.x = element_blank()) + 
    labs(x = "Measurements in College Scorecard data",
         y = "Relative importance in principle component",
         title = "Variables in PC1") +
    geom_bar(aes(x=Var1, y=value, fill=Var1), stat="identity") +
    coord_flip()

```

A lot of these are very important: SAT scores, the repayment data, the completion data.