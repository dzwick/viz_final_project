---
title: "Imputing Missing Values in the College Scorecard Data Via Very Simple Methods"
author: "Julia Silge"
date: "`r Sys.Date()`"
output:
  html_document:
    highlight: pygments
    theme: paper
    toc: yes
---

```{r, echo = FALSE, warning = FALSE}
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, dpi = 180)
options(width=100, dplyr.width = 150)
```

Let's work on imputing missing values in the [College Scorecard data]((https://collegescorecard.ed.gov/data/)) so we can do clustering. First off, let's try some very simple methods.

## Loading the Data and Counting the Missing Values

Let's open up the filtered, processed version of the College Scorecard data we have worked on together and settled on using at this point.

```{r}
library(readr)
colleges <- read_csv("data/scorecard_reduced_features.csv", na = c("NULL", "PrivacySuppressed"))
dim(colleges)
```

How many `NA` values do we have in each column?

```{r}
sapply(colleges, function(x) sum(is.na(x)))
sapply(colleges, function(x) mean(is.na(x)))
```

Most of those are not so bad, many around 15% or less. We have made so much improvement!

## Substituting a Median Value for Each `NA`

For a first try at imputing missing values, let's just try using the median for each column. What are the medians?

```{r}
library(plyr)
library(dplyr)
median_values <- colleges[,4:57] %>% 
    summarise_each(funs(median(., na.rm = TRUE)))

median_values
```

Now let's use these values in the data frame.

```{r}
colleges[4:57] <- colleges[4:57] %>%
    mutate_each(funs(if_else(is.na(.), median(., na.rm = TRUE), .)))
```

Did this work?

```{r}
sapply(colleges, function(x) mean(is.na(x)))
```

## Clustering Results with Median Imputation

Let's do k-means clustering now and see what kind of results we get. Let's use the broom package to explore how many clusters may be appropriate for this data set. For starters, let's try out 4 to 20 clusters.

```{r}
library(broom)
library(tidyr)
set.seed(1234)
kclusts <- data.frame(k=4:20) %>% 
    group_by(k) %>% 
    do(kclust = kmeans(colleges[4:57], .$k))
```

Now let's use `tidy`, `augment`, and `glance` to organize these various sets of k-means clusterings.

```{r}
clusters <- kclusts %>% group_by(k) %>% do(tidy(.$kclust[[1]]))
assignments <- kclusts %>% group_by(k) %>% do(augment(.$kclust[[1]], colleges[4:57]))
clusterings <- kclusts %>% group_by(k) %>% do(glance(.$kclust[[1]]))
```

Specifically, let's look at the total within sum of squares, saved in the `tot.withinss` column in `clusterings`. This shows us the variance within the clusters.

```{r, fig.width=8, fig.height=6}
library(ggfortify)
ggplot(clusterings, aes(k, tot.withinss)) +
    geom_line(color = "blue", alpha = 0.5, size = 2) +
    geom_point(size = 0.8)
```

Notice that the variance decreases with more clusters, but there is a "bend" or "elbow" around 12 clusters. This is around the same place we saw an elbow in the early subset of colleges (!), and tells us we don't gain much by going beyond 12 or so clusters. 

## Plotting Clustering Results

Let's look at how the colleges are distributed in the space of principal component 1 and 2, clustered with $k = 12$.

```{r, fig.width=10, fig.height=8}
set.seed(1234)
autoplot(kmeans(colleges[4:57], 12), data = colleges[4:57], size = 3, alpha = 0.5) + 
    ggtitle("K-Means Clustering of College Scorecard Data") +
    theme(legend.position="none")
```

Let's label a few of thes.

```{r, fig.width=10, fig.height=8}
library(ggrepel)
set.seed(1234)
my_kmeans <- kmeans(colleges[4:57], 12)
autoplot(my_kmeans, data = colleges[4:57], size = 3, alpha = 0.5) + 
    ggtitle("K-Means Clustering of College Scorecard Data") +
    theme(legend.position="none") +
    geom_text_repel(aes(PC1, PC2, 
                        color = factor(my_kmeans$cluster), 
                        label = ifelse(SAT_AVG_ALL > 1475,colleges$INSTNM,'')),
                    segment.color = NA)
```

Some of these clusters are very close and/or overlapping as viewed in this space (PC1 and PC2) but they may be more separated in other components/dimensions.

## Summary

I am really happy with these results; I am even super happy that, for example, University of Chicago and Harvey Mudd are in a separate cluster than MIT and Harvard. The clustering algorithm is really picking up on subtle differences in colleges here. Let's look, just for kicks, a that small and spread out cluster that CalTech is part of.

```{r}
colleges <- colleges %>% 
    mutate(cluster = my_kmeans$cluster) 

colleges %>%
    filter(cluster == 1) %>%
    select(INSTNM)
```

Huh, those are almost all medical schools, or schools where the medical school is very important.
